package com.ulb.code.wit.main

import org.apache.spark.SparkConf
import org.apache.spark.rdd.RDD
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.{ Seconds, StreamingContext, Time }
import org.apache.spark.sql.SQLContext
import org.apache.spark.SparkContext
import org.apache.spark.graphx.GraphXUtils
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.types.StringType
import org.apache.log4j.Logger
import org.apache.log4j.Level
import org.apache.spark.sql.Row
object StreamGraph {
//  Logger.getLogger("org").setLevel(Level.OFF)
//  Logger.getLogger("akka").setLevel(Level.OFF)
  val sparkConf = new SparkConf().setAppName("SqlNetworkWordCount").setMaster("local[*]")
//  val ssc = new StreamingContext(sparkConf, Seconds(2))
  val sc = new SparkContext(sparkConf)
  val sqlContext = new SQLContext(sc)
  val path = "C:\\Users\\Rohit\\ownCloud\\testdata\\groundTruth\\higgs-activity_Retweet.txt"

  def main(args: Array[String]) {
    val rdd = sc.textFile(path).flatMap { x => x.split(" ") }
    println(rdd.count())
    rdd.cache()
    rdd.persist()
    rdd.top(5).map(println)
    val data = rdd.filter { x => x.size > 5 }
    data.top(5).map(println)
    
  }
  def stream() {

    // Create the context with a 2 second batch size

    val schemaEdge = "src dst time"
    val schema =
      StructType(
        schemaEdge.split(" ").map(fieldName => StructField(fieldName, StringType, true)))

    import sqlContext.implicits._
    // Create a socket stream on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    // Note that no duplication in storage level only for running locally.
    // Replication necessary in distributed scenario for fault tolerance.
 //   val edgeStream = ssc.textFileStream("C:\\Users\\Rohit\\ownCloud\\testdata\\groundTruth\\higgs-activity_Retweet.txt")

    //    val edges = edgeStream.flatMap(_.split(" "))

    // Convert RDDs of the words DStream to DataFrame and run SQL query
   // edgeStream.foreachRDD { (rdd: RDD[String], time: Time) =>

      // Convert RDD[String] to RDD[case class] to DataFrame
    //  val edgeDF = sqlContext.createDataFrame(rdd.map(_.split(" ")).map(p => Row(p(0), p(1).trim, p(2).trim)), schema)

//    }

  //  ssc.start()
    //ssc.awaitTermination()

  }
}


// scalastyle:on println
